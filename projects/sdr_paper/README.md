
This directory contains code and Excel files that simulate various scenarios in
the SDR papers [1, 2] and the Neuron paper [3]. They are used to generate the numerical
results. The code also serves to verify the formulas in the papers.

The papers are available here:

[1] Ahmad, S., and Hawkins, J. (2015). *Properties of Sparse Distributed
Representations and their Application to Hierarchical Temporal Memory*.
*arXiv:1503.07469** Neurons and Cognition; Artificial Intelligence. Retrieved
from http://arxiv.org/abs/1503.07469

[2] Ahmad, S., and Hawkins, J. (2016). *How do neurons operate on sparse
distributed representations? A mathematical theory of sparsity, neurons and
active dendrites*. **arXiv:1601.00720** Neurons and Cognition; Artificial
Intelligence. Retrieved from http://arxiv.org/abs/1601.00720

[3] Hawkins, J., and Ahmad, S. (2016). Why Neurons Have Thousands of Synapses, a
Theory of Sequence Memory in Neocortex. Front. Neural Circuits 10.
doi:10.3389/fncir.2016.00023. PDF:
http://journal.frontiersin.org/article/10.3389/fncir.2016.00023/full


SDR Properties Calculations.xlsx
================================

This excel file contains the formulas in the SDR papers. You can plug in
different numbers and it tells you the result of various formulas.  Excel  uses
very high precision math, so you can compute the numbers for any  reasonable set
of parameters. (A much wider range than you can by simulations.) This
spreadsheet was used to compute the numbers in the examples and the tables in
the SDR paper.


sdr_math_neuron_paper.ipynb
===========================

An ipython notebook showing how to implement and compute the functions in
python using a symbolic math library.  This notebook also contains some of the
loops used to generate the numbers in the plots.


Optimal threshold
=================

compute_optimal_threshold.py computes averaged errors over a wide range of
values. The file plot_optimal_threshold.py plots the curve as a function of
theta and outlines the "optimal" area.


sdr_calculations[12].cpp
=====================

These C++ programs simulate SDR classification with various parameters.  In
particular it simulates the "Classifying a Set of Vectors" setup. You can set
values for M, n, and w and it will return the probability of a false match
for various values of theta. It computes these probabilities via simulations
(i.e. generating millions of random vectors and using a classifier to compute
whether there was a match or not).  It can also test false negatives with noise.

As such this can be used to verify that the math in the paper is accurate.

However, we can only really verify the math for relatively small parameters
this way. Many of the probabilities are insanely low and it is impossible to
verify via simulation.  Still if the math matches the simulation results
exactly for a wide range of parameters, we can be reasonably sure it is
accurate.

Currently you need to hard code the numbers in the C++ file,  compile and run.
Sorry about that. Here is an example run on my laptop:

For n=100, M=500, w=7 and setting nTrials to 1,000,000 it takes 10 minutes to
generate the following result:
```
Classification: Probability of false match for n=100, M=500, w=7
    Theta = 1 prob=1
    Theta = 2 prob=1
    Theta = 3 prob=0.964839
    Theta = 4 prob=0.134632
    Theta = 5 prob=0.00285
    Theta = 6 prob=1.7e-05
    Theta = 7 prob=0
```

These correspond quite closely to the formula in the paper. For example, for
theta=5 the formula leads to 0.002826477. For theta=4 the formula leads to
0.144691001.  For theta=6 the formula leads to 2.03654E-05

Remember  that  the formula is an upper bound on the classification error. It
will be less accurate as you get closer to 1.0. The simulations themselves will
be less accurate for really tiny numbers since we are running a finite number of
trials.  As an example, For theta=7 the  formula leads to 3.12352E-08 but we
need to run hundreds of millions of  trials to get that number through
simulations.

Plots
=====

There are various plotting scripts that generate images using plot.ly. Each
script contains the numbers calculated using one of the above methods. The image
below shows an example plot generated by plot_effect_of_n.py.

This graph illustrates the behavior of the main equation and the effects of cell
population and sparsity. The three solid curves show the rapid drop in error
rates as the number of cells n increases. Each curve shows a different sparsity
level. For example, if 128 out of 4000 cells are active (3.2% sparsity) the
error rate is a little higher than 10-12. The dashed line corresponds to an
activity level of 50%. The fact that the error corresponding to this condition
does not drop demonstrates that both sparsity and high dimensionality are
required to achieve low error rates. In all of these simulations, the number of
synapses s=24 and the dendritic threshold theta=12, corresponding to 50% noise
tolerance.

![Effect of n](https://github.com/numenta/nupic.research/blob/31f45e19903bacff308b36e07609d25059c63de0/projects/sdr_paper/images/effect_of_n.png)


Poirazi Neuron Model
====================

The poirazi_neuron_model directory contains experiment and plotting code for a
number of the figures in the paper.  Each experiment script has a corresponding
plotting script.  Experiment scripts contain information about what parameter
settings should be used to replicate the results in the paper.
Data must be manually imported into the plotting script, but
the plotting scripts already contain the data used to generate the versions of
the plots seen in the paper.

Note that these scripts write the result of every trial to file separately if
multiple cores are being used.  The data must then be merged before it can be
plotted.  In general, this is done by summing together the second and third
columns of all rows with the same first column.

Experiments
===========

run_correlation_false_positive_experiment: corresponds to false positive rate
vs data correlation figure.
Uses the data generation code in
htmresearch/frameworks/poirazi_neuron_model/data_tools.py to generate correlated
data, and then measures the ability of a Poirazi-style neuron to correctly
distinguish learned and unlearned patterns as a function of correlation.

run_dim_classification_experiment: corresponds to binary classification accuracy
vs number of cells figure.
Uses two Poirazi-style neurons to categorize data.  Computes accuracy as a
function of dimension, given a certain number of bits active.

run_HTM_classification_experiment: replicates the original experiment run by
Poirazi & Mel, with 50 cells competing to classify SDRs drawn from a
non-Gaussian distribution.  This experiment has no corresponding plot; we
directly report performance in the text.  Note that this employs an HTM-style
learning rule which is entirely separate from that used by Poirazi & Mel.

run_false_positive_experiment: corresponds to the false positive probability vs
segment size figure.
Uses a single Poirazi-style neuron to recognize data, and computes accuracy as
a function of segment length, given a certain SDR size and number of active bits
in the SDR.  Note that we only use a single neuron, and use a hard threshold on
each dendrite.  A false positive occurs if a dendrite has more active bits than
the threshold.

run_noise_experiment: corresponds to the frequence of false negative errors vs.
noise figure.  Uses a single Poirazi-style neuron to recognize noisy data.
Computes the probability of a pattern not being correctly recognized as a
function of noise level, given a certain dendrite activation threshold.

TM Experiments
==============

A set of experiments using the HTM temporal memory to illustrate the properties
of SDRs.  As before, each experiment script has a corresponding plotting script.
Experiment scripts contain information about what parameters settings should be
used to replicate the results in the paper.
Data must be manually copied into the plotting script, but the scripts currently
contain the data used to generate the figures in the paper.

Experiments
===========

run_tm_dim_experiment: corresponds to the TM accuracy vs. sparsity figure.  Uses
the TM to illustrate the importance of sparsity, by plotting the performance
ability of the TM to correctly recall a sequence even when trained to recognize
a very large number of sequences as a function of number of dimensions, with a
given number of active input bits.

run_tm_noise_experiment: corresponds to the sequence accuracy vs. noise figure.
Uses the TM to illustrate the effect of noise on the TM, and demonstrates that
even equations governing single SDRs can predict the performance of entire
populations of neurons.

run_tm_union_experiment: corresponds to the surprise vs. number of patterns in
union figure.  Uses the TM to demonstrate that the union property of SDRs allows
an unexpected transition to be recognized, even when a large number of possible
transitions are all simultaneously expected.


Note on using NuPIC Core
========================

The executable `sdr_calculations` is a C++ program that is a client of
`nupic.core`. As such it is an example of how to write C++ programs that use
that repository as an external library. I have tried to make it as simple as
possible.

There is a single, very simplistic, Makefile that shows how to compile and link
such code.  The Makefile assumes the environment variable `NUPIC_CORE` is
already setup to point to the root of your `nupic.core` repository.  It
assumes you have followed the `nupic.core` build instructions and have a
proper build in place.
